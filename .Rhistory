mergeddf %>%
ggplot(., aes(x = reorder(top_category,mean_achievement_ratio), y = mean_achievement_ratio)) +
geom_bar(stat="identity")  + coord_flip()+
ggtitle("Average Achievement Ratio per Top Category") +
xlab("Top Category") + ylab('Average Achievement Ratio') +theme_minimal()
backersdf <- kickstarter %>%
group_by(top_category) %>%
summarize(mean_backers_count = mean(backers_count,na.rm=TRUE))
backersdf %>%
ggplot(., aes(x = reorder(top_category,mean_backers_count), y = mean_backers_count)) +
geom_bar(stat="identity")  + coord_flip() +
ggtitle("Average Backer Count per Top Category") +
xlab("Top Category") + ylab('Average Backer Count') +theme_minimal()
top_1000 <- kickstarter %>%
filter(state=='successful') %>%
arrange(desc(achievement_ratio)) %>%
mutate(rank=row_number()) %>%
filter(rank<=1000)
bottom_1000 <- kickstarter %>%
filter(state=='failed') %>%
arrange(achievement_ratio) %>%
mutate(rank=row_number()) %>%
filter(rank<=1000)
#top1000
top_1000$blurb <- gsub("&amp", " ", top_1000$blurb)
top_1000$blurb <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", top_1000$blurb)
top_1000$blurb <- gsub("@\\w+", " ", top_1000$blurb)
top_1000$blurb <- gsub("[[:punct:]]", " ", top_1000$blurb)
top_1000$blurb <- gsub("[[:digit:]]", " ", top_1000$blurb)
top_1000$blurb <- gsub("http\\w+", " ", top_1000$blurb)
top_1000$blurb <- gsub("[ \t]{2,}", " ", top_1000$blurb)
top_1000$blurb <- gsub("ˆ\\s+|\\s+$", " ", top_1000$blurb)
top_1000$blurb <- gsub("\n", " ", top_1000$blurb)
top<- top_1000$blurb
top_1000_tm <- VCorpus(VectorSource(top))
top_1000_tm <- tm_map(top_1000_tm,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')))
top_1000_tm <- tm_map(top_1000_tm, PlainTextDocument)
top_1000_tm <- tm_map(top_1000_tm, removePunctuation)
top_1000_tm <- tm_map(top_1000_tm, content_transformer(tolower))
top_1000_tm <- tm_map(top_1000_tm, removeNumbers)
top_1000_tm <- tm_map(top_1000_tm, removeWords, stopwords("english"))
top_1000_dtm <- DocumentTermMatrix(top_1000_tm)
top_1000_m <- as.matrix(top_1000_dtm)
top_1000_td <- tidy(top_1000_dtm)
top_lemmatized <- top_1000_td %>%
mutate(lemma = lemmatize_words(term))
top_lemmatized <- top_lemmatized %>% group_by(lemma) %>% summarize(count = sum(count))
#bottom 1000
bottom_1000$blurb <- gsub("&amp", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("@\\w+", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("[[:punct:]]", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("[[:digit:]]", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("http\\w+", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("[ \t]{2,}", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("ˆ\\s+|\\s+$", " ", bottom_1000$blurb)
bottom_1000$blurb <- gsub("\n", " ", bottom_1000$blurb)
bottom <- bottom_1000$blurb
bottom_1000_tm <- VCorpus(VectorSource(bottom))
bottom_1000_tm <- tm_map(bottom_1000_tm,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')))
bottom_1000_tm <- tm_map(bottom_1000_tm, PlainTextDocument)
bottom_1000_tm <- tm_map(bottom_1000_tm, removePunctuation)
bottom_1000_tm <- tm_map(bottom_1000_tm, content_transformer(tolower))
bottom_1000_tm <- tm_map(bottom_1000_tm, removeNumbers)
bottom_1000_tm <- tm_map(bottom_1000_tm, removeWords, stopwords("english"))
bottom_1000_dtm <- DocumentTermMatrix(bottom_1000_tm)
bottom_1000_m <- as.matrix(bottom_1000_dtm)
bottom_1000_td <- tidy(bottom_1000_dtm)
bottom_lemmatized <- bottom_1000_td %>%
mutate(lemma = lemmatize_words(term))
bottom_lemmatized <- bottom_lemmatized %>% group_by(lemma) %>% summarize(count = sum(count))
color_pal <- turbo(n = 5)
set.seed(1234)
wordcloud(top_lemmatized$lemma, top_lemmatized$count,
min.freq=15,random.order=FALSE,colors = color_pal)
color_pal <- turbo(n = 5)
set.seed(1234)
wordcloud(bottom_lemmatized$lemma, bottom_lemmatized$count,
min.freq=15,random.order=FALSE,colors = color_pal)
together_lemmatized <- merge(bottom_lemmatized,top_lemmatized,by="lemma")
colnames(together_lemmatized)<-c('lemma','bottom','top')
together_lemmatized$total <- together_lemmatized$bottom + together_lemmatized$top
pyramid<-together_lemmatized %>%
arrange(desc(total)) %>%
mutate(rank=row_number()) %>%
filter(rank<=20)
pyramid.plot(pyramid$bottom, pyramid$top,
labels = pyramid$lemma,
gap = 10,
top.labels = c("Bottom", " ", "Top"),
main = "Words in Common",
laxlab = NULL,
raxlab = NULL,
unit = NULL,
labelcex=0.5)
require(quanteda.textstats)
top_readability <- textstat_readability(c(top),
measure=c('Flesch','Flesch.Kincaid',
'meanSentenceLength','meanWordSyllables'))
bottom_readability <- textstat_readability(c(bottom),
measure=c('Flesch','Flesch.Kincaid',
'meanSentenceLength','meanWordSyllables'))
top_readability <- top_readability %>% mutate(rn = row_number())
top_1000 <- top_1000 %>% mutate(rn = row_number())
top_total <- merge(top_readability,top_1000,by='rn')
top_total$type<-'top'
bottom_readability <- bottom_readability %>% mutate(rn = row_number())
bottom_1000 <- bottom_1000 %>% mutate(rn = row_number())
bottom_total <- merge(bottom_readability,bottom_1000,by='rn')
bottom_total$type<-'bottom'
total <- rbind(top_total,bottom_total)
ggplot(top_total) + geom_point(aes(y=Flesch.Kincaid,x=backers_count,colour=meanSentenceLength),size=5)+
ggtitle("Flesch Kincaid Score per Backer Count") +
xlab("Backer Count") + ylab('Flesch Kincaid Score') +theme_minimal() +labs(color='Mean Sentence Length')+
theme(axis.title.x = element_text(size = 15),
axis.title.y = element_text(size=15),
title = element_text(size=20)) + scale_color_gradient(low="blue", high="red")
setwd("~/Documents/GitHub/assignment-3---kickstarter-gamzebilsen/")
pos <- read.table("positive-words.txt", as.is=T)
neg <- read.table("negative-words.txt", as.is=T)
pos[1:15,]
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
set.seed(12345)
kickstarter <- sample_n(kickstarter, 2000)
kickstarter$blurb <- gsub("&amp", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("@\\w+", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("[[:punct:]]", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("[[:digit:]]", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("http\\w+", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("[ \t]{2,}", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("ˆ\\s+|\\s+$", " ", kickstarter$blurb)
kickstarter$blurb <- gsub("\n", " ", kickstarter$blurb)
kickstarter2<- kickstarter$blurb
random2K_tone <- data.frame(mapply(sentiment,kickstarter2))
random2K_tone <- data.frame(random2K_tone)
random2K_tone[is.na(random2K_tone)] <- 0
colnames(random2K_tone) <- c('tone')
random2K_tone <- random2K_tone %>% mutate(rn = row_number())
kickstarter <- kickstarter %>% mutate(rn = row_number())
random2K <- merge(random2K_tone,kickstarter,by='rn')
mean_ar <- random2K %>%
group_by(tone) %>%
summarize(mean_achievement_ratio = mean(achievement_ratio))
ggplot(mean_ar,aes(x=tone,y=mean_achievement_ratio)) +
geom_smooth(method='loess',color='black')+
ggtitle("Achievement Ratio and Tone of Blurb") +
xlab("Tone of Text") + ylab('Achievement Ratio') +theme_minimal() +
theme(axis.title.x = element_text(size = 15),
axis.title.y = element_text(size=15),
title = element_text(size=20)) + scale_color_gradient(low="blue", high="red")
random2K$blurb <- gsub("&amp", " ", random2K$blurb)
random2K$blurb <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", random2K$blurb)
random2K$blurb <- gsub("@\\w+", " ", random2K$blurb)
random2K$blurb <- gsub("[[:punct:]]", " ", random2K$blurb)
random2K$blurb <- gsub("[[:digit:]]", " ", random2K$blurb)
random2K$blurb <- gsub("http\\w+", " ", random2K$blurb)
random2K$blurb <- gsub("[ \t]{2,}", " ", random2K$blurb)
random2K$blurb <- gsub("ˆ\\s+|\\s+$", " ", random2K$blurb)
random2K$blurb <- gsub("\n", " ", random2K$blurb)
random2Kdf <- random2K$blurb
random2Kdf_tm <- VCorpus(VectorSource(random2Kdf))
random2Kdf_tm <- tm_map(random2Kdf_tm,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')))
random2Kdf_tm <- tm_map(random2Kdf_tm, PlainTextDocument)
random2Kdf_tm <- tm_map(random2Kdf_tm, removePunctuation)
random2Kdf_tm <- tm_map(random2Kdf_tm, content_transformer(tolower))
random2Kdf_tm <- tm_map(random2Kdf_tm, removeNumbers)
random2Kdf_tm <- tm_map(random2Kdf_tm, removeWords, stopwords("english"))
random2Kdf_tm <- DocumentTermMatrix(random2Kdf_tm)
random2Kdf_tm <- tidy(random2Kdf_tm)
random2K_lemmatized <- random2Kdf_tm %>%
mutate(lemma = lemmatize_words(term))
bing<-get_sentiments('bing')
random2K_lemma_tone2<-merge(random2K_lemmatized, bing,by.x='lemma',by.y='word')
# Below code from: https://gist.github.com/rer145/8f31af53a9a8339dddbef93fd10e86ce
words<-random2K_lemma_tone2%>%
group_by(lemma)%>%
summarize(count=n(), sentiment=first(sentiment))%>%
arrange(count)
matrix<-acast(words, lemma~sentiment, value.var='count', fill=0)
comparison.cloud(matrix, colors=c('red', 'green'))
nrc <- get_sentiments("nrc")
random2K_lemma_nrc<-merge(random2K_lemmatized, nrc,by.x='lemma',by.y='word')
fear <- random2K_lemma_nrc %>%
filter(sentiment=='fear')
joy <- random2K_lemma_nrc %>%
filter(sentiment=='joy')
anticipation <- random2K_lemma_nrc %>%
filter(sentiment=='anticipation')
sadness <- random2K_lemma_nrc %>%
filter(sentiment=='sadness')
feartoMatch <- fear$term
joytoMatch <- joy$term
anticipationtoMatch <- anticipation$term
sadnesstoMatch <- sadness$term
#calculating the number of words that fit the 4 categories for each blurb
random2K$fearcount <- stri_count_regex(random2K$blurb, paste(feartoMatch, collapse="|"))
random2K$joycount <- stri_count_regex(random2K$blurb, paste(joytoMatch, collapse="|"))
random2K$anticipationcount <- stri_count_regex(random2K$blurb, paste(anticipationtoMatch, collapse="|"))
random2K$sadnesscount <- stri_count_regex(random2K$blurb, paste(sadnesstoMatch, collapse="|"))
#calculating the percentage of each category per blurb
random2K$total_words <- sapply(random2K$blurb, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
random2K$percent_fear <- random2K$fearcount / random2K$total_words *100
random2K$percent_joy <- random2K$joycount / random2K$total_words*100
random2K$percent_anticipation <- random2K$anticipationcount / random2K$total_words *100
random2K$percent_sadness <- random2K$sadnesscount / random2K$total_words *100
ggplot(random2K,aes(fearcount,achievement_ratio)) +
geom_col()  +
ggtitle("Fear Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Fear Count') +theme_minimal() +coord_flip()+ ylim(0,200000)+xlim(-1,6)
ggplot(random2K,aes(joycount,achievement_ratio)) +
geom_col()  +
ggtitle("Joy Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Joy Count') +theme_minimal() +coord_flip()+ ylim(0,200000)+xlim(-1,6)
ggplot(random2K,aes(anticipationcount,achievement_ratio)) +
geom_col()  +
ggtitle("Anticipation Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Anticipation Count') +theme_minimal() +coord_flip() + ylim(0,200000) +xlim(-1,6)
ggplot(random2K,aes(sadnesscount,achievement_ratio)) +
geom_col()  +
ggtitle("Sadness Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Sadness Count') +theme_minimal()  +coord_flip() + ylim(0,200000)+xlim(-1,6)
read.delim('nrc.txt')
setwd("~/Documents/GitHub/assignment-3---kickstarter-gamzebilsen/")
read.delim('nrc.txt')
#nrc <- get_sentiments("nrc")
setwd("~/Documents/GitHub/assignment-3---kickstarter-gamzebilsen/")
nrc <- read.delim('nrc.txt')
colnames(nrc) <- c('name','sentiment','exist')
nrc %>% filter(exist==1)
#nrc <- get_sentiments("nrc") I was going to use this but it wouldn't knit so I downloaded it from https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm and used the text file, filtering to just the 1's to get the sentiments.
setwd("~/Documents/GitHub/assignment-3---kickstarter-gamzebilsen/")
nrc <- read.delim('nrc.txt')
colnames(nrc) <- c('name','sentiment','exist')
nrc <- nrc %>% filter(exist==1)
#nrc <- get_sentiments("nrc") I was going to use this but it wouldn't knit so I downloaded it from https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm and used the text file, filtering to just the 1's to get the sentiments.
setwd("~/Documents/GitHub/assignment-3---kickstarter-gamzebilsen/")
nrc <- read.delim('nrc.txt')
colnames(nrc) <- c('word','sentiment','exist')
nrc <- nrc %>% filter(exist==1)
#nrc <- get_sentiments("nrc") I was going to use this but it wouldn't knit so I downloaded it from https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm and used the text file, filtering to just the 1's to get the sentiments.
random2K_lemma_nrc<-merge(random2K_lemmatized, nrc,by.x='lemma',by.y='word')
fear <- random2K_lemma_nrc %>%
filter(sentiment=='fear')
joy <- random2K_lemma_nrc %>%
filter(sentiment=='joy')
anticipation <- random2K_lemma_nrc %>%
filter(sentiment=='anticipation')
sadness <- random2K_lemma_nrc %>%
filter(sentiment=='sadness')
feartoMatch <- fear$term
joytoMatch <- joy$term
anticipationtoMatch <- anticipation$term
sadnesstoMatch <- sadness$term
#calculating the number of words that fit the 4 categories for each blurb
random2K$fearcount <- stri_count_regex(random2K$blurb, paste(feartoMatch, collapse="|"))
random2K$joycount <- stri_count_regex(random2K$blurb, paste(joytoMatch, collapse="|"))
random2K$anticipationcount <- stri_count_regex(random2K$blurb, paste(anticipationtoMatch, collapse="|"))
random2K$sadnesscount <- stri_count_regex(random2K$blurb, paste(sadnesstoMatch, collapse="|"))
#calculating the percentage of each category per blurb
random2K$total_words <- sapply(random2K$blurb, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
random2K$percent_fear <- random2K$fearcount / random2K$total_words *100
random2K$percent_joy <- random2K$joycount / random2K$total_words*100
random2K$percent_anticipation <- random2K$anticipationcount / random2K$total_words *100
random2K$percent_sadness <- random2K$sadnesscount / random2K$total_words *100
ggplot(random2K,aes(fearcount,achievement_ratio)) +
geom_col()  +
ggtitle("Fear Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Fear Count') +theme_minimal() +coord_flip()+ ylim(0,200000)+xlim(-1,6)
ggplot(random2K,aes(joycount,achievement_ratio)) +
geom_col()  +
ggtitle("Joy Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Joy Count') +theme_minimal() +coord_flip()+ ylim(0,200000)+xlim(-1,6)
ggplot(random2K,aes(anticipationcount,achievement_ratio)) +
geom_col()  +
ggtitle("Anticipation Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Anticipation Count') +theme_minimal() +coord_flip() + ylim(0,200000) +xlim(-1,6)
ggplot(random2K,aes(sadnesscount,achievement_ratio)) +
geom_col()  +
ggtitle("Sadness Count and Achievement Ratio") +
ylab("Achievement Ratio") + xlab('Sadness Count') +theme_minimal()  +coord_flip() + ylim(0,200000)+xlim(-1,6)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total')
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total.csv')
setwd("~/Documents")
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
setwd("~/Documents/GitHub/Group_J_Climate_Economics")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total.csv')
#setwd("~/Documents/GitHub/Group_J_Climate_Economics")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('/final_total.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
#null = read.csv('/analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
null.head(20)
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
head(null)
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
null
library(ggplot2)
ggplot(df, aes(x = 'Year', y = 'Total Deaths')) +
geom_line()
library(ggplot2)
ggplot(df, aes(x = Year, y = Total Deaths)) +
library(ggplot2)
ggplot(df, aes(x = Year, y = (Total Deaths))) +
df %>% rename_all(function(x) gsub(" ", "_", x))
colnames(df) %>% rename_all(function(x) gsub(" ", "_", x))
colnames(df)#%>% rename_all(function(x) gsub(" ", "_", x))
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
null
colnames(df)
library(ggplot2)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_line()
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
null
colnames(df)
library(ggplot2)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_line()
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
null
colnames(df)
library(ggplot2)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_line()
setwd("~/Documents/GitHub/Group_J_Climate_Economics/")
null = read.csv('analysis/initial_analysis/totalnullvaluescountry.csv')
df =  read.csv('final_total.csv')
head(df)
null
colnames(df)
library(ggplot2)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_line()
library(ggplot2)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_point()
library(ggplot2)
library(tidyr)
df %>% groupby(Year)
library(ggplot2)
library(tidyr)
df %>%
group_by(Year)
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year)
ggplot(df, aes(x = Year, y = Total.Deaths)) +
geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths))
#ggplot(df, aes(x = Year, y = Total.Deaths)) +
#  geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE))
#ggplot(df, aes(x = Year, y = Total.Deaths)) +
#  geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE)) %>%
ggplot(., aes(x = Year, y = Total.Deaths)) +
geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE)) %>%
ggplot(., aes(x = Year, y = Total.Deaths)) %>%
geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE)) %>%
ggplot(., aes(x = Year, y = Total.Deaths)) +
geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_point()
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_line()
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_line()
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_line() + geom_line(aes(x=Year, y=sum_affected))
year_death
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE),
affected_to_death = sum(Total.Deaths/Total.Affected,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_line() + geom_line(aes(x=Year, y=sum_affected))
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE),
affected_to_death = sum(Total.Deaths/Total.Affected,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_death)) +
geom_line() + geom_line(aes(x=Year, y=sum_affected))
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE),
affected_to_death = sum(Total.Deaths/Total.Affected,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = sum_affected)) +
geom_line() #+ geom_line(aes(x=Year, y=sum_affected))
library(ggplot2)
library(tidyr)
library(dplyr)
year_death <- df %>%
group_by(Year) %>%
summarize(sum_death = sum(Total.Deaths,na.rm=TRUE),
sum_affected = sum(Total.Affected, na.rm=TRUE),
affected_to_death = sum(Total.Deaths/Total.Affected,na.rm=TRUE))
ggplot(year_death, aes(x = Year, y = affected_to_death)) +
geom_line() #+ geom_line(aes(x=Year, y=sum_affected))
